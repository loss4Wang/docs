[{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-12","objectID":"/introduction2la_l2/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"1. Mind Map 2. Reading Notes(2.2-2.3) ","date":"2022-03-12","objectID":"/introduction2la_l2/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"2.2 The Idea of Elimination A systematic way to solve linear equations: elimination GOAL: produce an upper triangular system The system solved from the bottom upwards: back substitution Pivot : first nonzero in the row that does the elimination Multiplier : (entry to eliminate) divided by (pivot) ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Breakdown of Elimination Failure: The method might ask us to divide by zero. Permanent failure with no solution. Failure with infinitely many solutions. Elimination leads to an equation $0 \\neq 0$ (no solution) or 0 = 0 (many solutions) Success comes with n pivots. But we may have to exchange the n equations. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Three Equations in Three Unknowns Goal: Forward elimination is complete from A to U. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Elimination from A to U Column 1. Use the first equation to create zeros below the first pivot. Column2. Use the new equation2 to create zeros below the second pivot. Columns 3 to n. Keep going to find all n pivots and the upper triangular U. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"2.3 Elimination Using Matrices ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Matrices times Vectors and Ax = b $Ax=b$ Ax is a combination of columns of A Components of Ax are dot products with rows of A. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Matrix Form of One Elimination Step $Ax=b: \\begin{bmatrix}2\u00264\u0026-2 \\cr 4\u00269\u0026-3 \\cr -2\u0026-3\u00267 \\end{bmatrix} \\begin{bmatrix} -1 \\cr 2 \\cr 2 \\end{bmatrix} =\\begin{bmatrix}2 \\cr 8 \\cr 10 \\end{bmatrix}$ First step: Subtract 2*Row1 from Row2 Elimination matrix is $E = \\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix}$ $b_{new} = Eb$ $\\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix} \\begin{bmatrix} 2 \\cr 8 \\cr 10 \\end{bmatrix} =\\begin{bmatrix}2 \\cr 4 \\cr 10 \\end{bmatrix}$ $\\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix} \\begin{bmatrix}b_1 \\cr b_2 \\cr b_3 \\end{bmatrix} = \\begin{bmatrix}b_1 \\cr -2b_1+b_2 \\cr b_3 \\end{bmatrix}$ I: identity matrix $\\begin{bmatrix}1\u00260\u00260 \\cr 0\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix}$ E :elementary matrix or elimination matrix $E_{ij}$ has extra nonzero entry $-l$ in the i,j position. Then $E_{ij}$ subtracts a multiple $l$ of row j from row i. $E_{31} = \\begin{bmatrix}1\u00260\u00260 \\cr 0\u00261\u00260 \\cr -l\u00260\u00261 \\end{bmatrix}$ **The purpose of $E_{31}$ is to produce a zero in the ( 3, 1) position of the matrix.** Products and inverses are especially clear for E‚Äôs. It is those two ideas that the book will use. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Matrix Multiplication Q: How do we multiply two matrices? $E(Ax) =Eb \\quad{also}\\quad (EA)x = Eb$ Associative law is true Commutative law is false E on the rights acts on the columns of A E on the left acts on the rows of A Matrix multiplication: $AB = A [b_1,b_2,b_3]=[Ab_1,Ab_2,Ab_3]$ ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Matrix $P_{ij}$ **for a Row Exchange** Permutation Matrix A row exchange is needed when zero is in the pivot position. $$ \\begin{bmatrix}1\u00260\u00260 \\cr 0\u00260\u00261 \\cr 0\u00261\u00260 \\end{bmatrix} \\begin{bmatrix}2\u00264\u00261 \\cr 0\u00260\u00263 \\cr 0\u00266\u00265 \\end{bmatrix} = \\begin{bmatrix}2\u00264\u00261 \\cr 0\u00266\u00265 \\cr 0\u00260\u00263 \\end{bmatrix} $$ Row Exchange Matrix $P_{ij}$ is the identity matrix with rows i and j reversed. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:4","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Augmented Matrix $$ \\text{Augmented matrix} [A\\quad b] = \\begin{bmatrix} 2\u00264\u0026-1\u00262 \\cr 4\u00269\u0026-3\u00264 \\cr -2 \u0026-3\u00267\u002610 \\end{bmatrix} $$ ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:5","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-10","objectID":"/introduction2la_l1/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"Lecture 1: The geometry of linear equations 1. Mind Map 2. Reading Notes (1.1-2.1) Chapter 1 Introduction to Vectors $$ \\text{Linear combination} \\quad c\\vec{v}+ d\\vec{w}= c \\begin{bmatrix} 1\\cr 1\\end{bmatrix} +d \\begin{bmatrix}2\\cr 3\\end{bmatrix} =\\begin{bmatrix}c+2d\\cr c+3d\\end{bmatrix} $$ 1.1 Vector addition $v+w$ and linear combinations $cv+dw$. 1.2 The dot product $v ¬∑ w$ of two vectors and the length $\\lVert v \\rVert = \\sqrt{v \\cdot v}$ 1.3 Matrices A, linear equations $Ax=b$, solutions $x = A^{-1} b.$ ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.1 Vectors and Linear Combinations Vector Addition Scalar Multiplication ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.2 Lengths and Dot Products Dot product / Inner Product is the number of $\\vec{v}\\cdot \\vec{w}$ $\\vec{v}= (v_1,v_2), \\vec{w}= (w_1,w_2)$ $\\vec{v}\\cdot \\vec{w} = v_1w_1 +v_2w_2$ Dot product = 0 ‚Üí Perpendicular vectors Order makes no difference. Lengths and Unit Vectors length = $\\lVert v \\rVert = \\sqrt{v \\cdot v} = (v_1^2 + v_2^2 \\cdots +v_n^2)^{1/2}$ unit vector u is a vector whose length equals one Unit vector $u = v / \\lVert v\\rVert$ Cosine Formula: If v and w are nonzero vectors then $\\frac{v \\cdot w}{\\lVert v\\rVert\\lVert w\\rVert} = cos\\theta$ Schwarz Inequality $|v \\cdot w| \\leq \\lVert v\\rVert\\lVert w\\rVert$ Triangle Inequality $\\lVert v+w\\rVert \\leq \\lVert v\\rVert + \\lVert w\\rVert$ Geometric mean $\\leq$ Arithmetic mean : $\\sqrt{xy} \\leq \\frac{x+y}{2}$ *Cosine Formula: $cosine = v\\prime * w / (norm(v)norm(w))$ The arc cosine: angle = acos(cosine) ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.3 Matrices Matrix times vector: Combination b of columns of A (matrix A acts on the vector x) $$ Ax = \\begin{bmatrix}1\u00260\u00260 \\cr -1\u00261\u00260\\cr 0\u0026-1\u00261 \\end{bmatrix} \\begin{bmatrix}x_1 \\cr x_2\\cr x_3 \\end{bmatrix} = \\begin{bmatrix}x_1 \\cr x_2-x_1 \\cr x_3-x_2 \\end{bmatrix} $$ Ax is also dot products with rows $$ Ax = \\begin{bmatrix}1\u00260\u00260 \\cr -1\u00261\u00260\\cr 0\u0026-1\u00261 \\end{bmatrix} \\begin{bmatrix}x_1 \\cr x_2\\cr x_3 \\end{bmatrix} = \\begin{bmatrix}x_1 \\cr x_2-x_1 \\cr x_3-x_2 \\end{bmatrix}= \\begin{bmatrix}(1,0,0)\\cdot(x_1,x_2,x_3) \\cr (-1,1,0)\\cdot(x_1,x_2,x_3) \\cr (0,-1,1)\\cdot(x_1,x_2,x_3) \\end{bmatrix} $$ Linear Equations new viewpoint: Q: Which combination of u, v, w produces a particular vector b? Inverse problem: how to find the input x that gives the desired output b = Ax $\\text{Equations: }Ax=b\\quad \\text{Solution: }x=A^{-1}b$ matrix A is ‚Äúinvertible‚Äù üí° The Inverse Matrix ???: centered difference Worked examples **1.3C** Independence and Dependence Independent columns: Ax=0 has one solution. A is an invertible matrix. Dependent columns: Cx = 0 has many solutions. C is a singular matrix. ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"Chapter 2 Solving Linear Equations ","date":"2022-03-10","objectID":"/introduction2la_l1/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"2.1 Vectors and Linear Equations Two equations, two unknowns Row Picture Column Picture Coefficient matrix The Matrix Form of the Equations Multiplication by rows. (dot products) $Ax = \\begin{bmatrix}(row1)\\cdot x \\cr (row2)\\cdot x \\cr (row3)\\cdot x \\end{bmatrix}$ Multiplication by columns (combination of columns) $Ax = x(column1) +y(column2) +z(column3)$ ","date":"2022-03-10","objectID":"/introduction2la_l1/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Watching"],"content":"Notes on Youtube channel: AI coffe break Letitia.","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"How to check if a neural network has learned a specific phenomenon? ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:0:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"Question: How do we check if a neural network trained on task A has learned a phenomenon specific to task B? To NLP, the task is called probing/diagnostic classifier/probing task. ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:1:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"How do we do probing? Freeze the parameters of Input layer and Hidden layers, Replace the last layers to Probing layers, Train the Probing layers on a small dataset If task A is Masked word prediction and task B is Sentiment analysis, then: ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:2:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"Most recently research paper: Information-Theoretic Probing with Minimum Description Length on ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:3:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"More info Link: https://www.youtube.com/watch?v=fL22NAtMNYo ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:4:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Studying"],"content":"A three moths schedule for Machine Learning Foundations.","date":"2022-01-25","objectID":"/3_month_ml/","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"Build Your Machine Learning on the Rock A three moths schedule for Machine Learning Foundations. ","date":"2022-01-25","objectID":"/3_month_ml/:0:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"1. Introduction Nowadays Machine learning and Deep learning are widely used in all subjects, both industry and research organizations take huge effort to apply these methods. I major in Information Management and Information System for my undergraudate study, and had some research experience related to Natural language Processing. I‚Äôm quite excited to study in the area of AI, especially in NLP/CLÔºåsince there are so many great contributors and huge progressed they‚Äôve achieved. More is Less and Less is More! I believe that no matter how fast this field is growing, the most important thing we can do is to build a good foundation if we want to dive into it. ","date":"2022-01-25","objectID":"/3_month_ml/:1:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2. Planning Thanks to an UP(NLP‰ªéÂÖ•Èó®Âà∞ÊîæÂºÉ) share his great learning experience bilibili, I decided to follow his guide and start my 3 moths ML schedule. There 6 modules in this guide for about 3 months. ","date":"2022-01-25","objectID":"/3_month_ml/:2:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.1 Prerequisites Week 1: Python Week 2: Python for data analysis Week3: Math(Linear Algebra, Calculus, Probability) ","date":"2022-01-25","objectID":"/3_month_ml/:2:1","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.2 ML Theory Book : Statistical learning methods by Hang Li Must-learn chapter Introduction Perceptron Naive Bayes Decision Tree Logistic Regression and MaxEnt Boosted Trees XGBoost Reference Blog: Jianping Liu‚Äôs Blog Principle Can derive formulas by yourself. Don‚Äòt read every chapter in this book except for the must-learns'. Don‚Äòt reinvent wheels ","date":"2022-01-25","objectID":"/3_month_ml/:2:2","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.3 ML Practice Alibaba Cloud TIANCHI Industrial steam volume forecast Repeat purchase prediction for Tmall users O2O Coupon Forecast Ali cloud security malicious program detection ","date":"2022-01-25","objectID":"/3_month_ml/:2:3","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.4 DL Theory CV - cs231n P1-P22 HW1,HW2 NLP - cs224n P1-P5 \u0026 P8,P9,P11 HW Pytorch Learning Bilibili Âàò‰∫åÂ§ß‰∫∫ A repo ","date":"2022-01-25","objectID":"/3_month_ml/:2:4","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.5 DL Practice NLP Ali Cloud Tianchi: News Text Categories CV Ali Cloud Tianchi: Streetscape symbol recognition ","date":"2022-01-25","objectID":"/3_month_ml/:2:5","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.6 Interview Questions Book: The Quest for Machine Learning ","date":"2022-01-25","objectID":"/3_month_ml/:2:6","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"3. Note Every time I have finished some in this schedule, I‚Äôll post it to my blog. ","date":"2022-01-25","objectID":"/3_month_ml/:3:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"}]