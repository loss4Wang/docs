[{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-20","objectID":"/introduction2la_l4/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L4 Note","uri":"/introduction2la_l4/"},{"categories":["Studying"],"content":"1. Mind Map 2. Reading Notes (2.6) ","date":"2022-03-20","objectID":"/introduction2la_l4/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L4 Note","uri":"/introduction2la_l4/"},{"categories":["Studying"],"content":"2.6 Elimination = Factorization : A = LU $(E_{32}E_{31}E_{21})A = U \\quad\\text{becomes } A = (E_{21}^{-1}E_{31}^{-1}E_{32}^{-1})U \\text{which is } A = LU.$ ","date":"2022-03-20","objectID":"/introduction2la_l4/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L4 Note","uri":"/introduction2la_l4/"},{"categories":["Studying"],"content":"Explanation and Examples Special Pattern $A = \\begin{bmatrix} 1 \u0026 1 \u0026 0 \u0026 0 \\cr 1 \u0026 2 \u0026 1 \u0026 0 \\cr 0 \u0026 1 \u0026 2 \u0026 1 \\cr 0 \u0026 0 \u0026 1 \u0026 2 \\end{bmatrix} = LU \\begin{bmatrix} 1 \u0026 \u0026 \u0026 \\cr 1 \u0026 1 \u0026 \u0026 \\cr 0 \u0026 1 \u0026 1 \u0026 \\cr 0 \u0026 0 \u0026 1 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 1 \u0026 0 \u0026 0 \\cr \u0026 1 \u0026 1 \u0026 0 \\cr \u0026 \u0026 1 \u0026 1 \\cr \u0026 \u0026 \u0026 1 \\end{bmatrix}$ Assume no row exchanges, we can predict zeros in L and U: When a row of A starts with zeros, so does that row of L. When a column of A starts with zeros, so does that column of U. Key reason why A equals LU (Row 3 of A)= $l_{31}$(Row 1 of U) + $l_{32}$(Row 2 of U) + 1(Row 3 of U). This is exactly row 3 of A = LU. That row of L holds $l_{31}$, $l_{32}$ , 1. All rows look like this, whatever the size of A. With no row exchanges, we have A= LU. üí° Why explain this? Better balance from LDU A = LU is ‚Äúunsymmetric‚Äù, because U has the pivots on its diagonal where L has 1‚Äôs. Divide U by a diagonal matrix D that contains the pivots. That leaves a new triangular matrix with l‚Äôs on the diagonal. $$ LU =\\begin{bmatrix} 1 \u0026 0 \\cr 3 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 2 \u0026 8 \\cr 0 \u0026 5 \\end{bmatrix} \\text{splits further into } LDU= \\begin{bmatrix} 1 \u0026 0 \\cr 3 \u0026 1 \\end{bmatrix} \\begin{bmatrix} 2 \u0026 0 \\cr 0 \u0026 5 \\end{bmatrix} \\begin{bmatrix} 1 \u0026 4 \\cr 0 \u0026 1 \\end{bmatrix} $$ ","date":"2022-03-20","objectID":"/introduction2la_l4/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L4 Note","uri":"/introduction2la_l4/"},{"categories":["Studying"],"content":"One Square System = Two Triangular Systems Matrix L contains memory of Gaussian elimination. How do we use it in solving $Ax = b$? Forward (elimination) and backward (substitution) System $Ax=b$ is factored into two triangular systems Solve $Lc = b$ and then solve $Ux=c$ Example: $$ Ax = b \\quad \\begin{cases}u + 2v \u0026= 5 \\cr 4v + 9v \u0026=21 \\end{cases} $$ Forward elimination $$ Ux = c \\quad \\begin{cases}u + 2v \u0026= 5 \\cr v \u0026=1 \\end{cases} $$ $Lc =b$ The lower triangular system $\\begin{bmatrix} 1 \u0026 0 \\cr 4 \u0026 1 \\end{bmatrix} \\begin{bmatrix} c_1 \\cr c_2 \\end{bmatrix} =\\begin{bmatrix} 5 \\cr 21 \\end{bmatrix}$ gave $c =\\begin{bmatrix} 5 \\cr 1 \\end{bmatrix}$ $Ux=c$ The upper triangular system $\\begin{bmatrix} 1 \u0026 2 \\cr 0 \u0026 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\cr x_2 \\end{bmatrix} =\\begin{bmatrix} 5 \\cr 1 \\end{bmatrix}$ gives $x = \\begin{bmatrix} 3 \\cr 1 \\end{bmatrix}$ ","date":"2022-03-20","objectID":"/introduction2la_l4/:1:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L4 Note","uri":"/introduction2la_l4/"},{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-14","objectID":"/introduction2la_l3/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"1. Mind Map 2. Reading Notes (2.4-2.5) ","date":"2022-03-14","objectID":"/introduction2la_l3/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"2.4 Rules for Matrix Operations ","date":"2022-03-14","objectID":"/introduction2la_l3/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"4 ways to find AB. (matrix multiplication) Take the dot product of each row of A with each column of B. Each column of AB is a combination of the columns of A. (Matrix A times every column of B) Every row of AB is a combination of the rows of B. (Every row of A times matrix B) Multiply columns 1 to n of A times rows 1 ton of B. Add those matrices. ","date":"2022-03-14","objectID":"/introduction2la_l3/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"The Laws for Matrix Operations Addition Law commutative law distributive law associative law Multiplication Law associative law for ABC distributive law from the left distributive law from the right Law of exponents $A^{P} = AAA\\cdots A\\text{(p factors)}$ $(A^p)(A^q)=A^{p+q}$ $(A^p)^q=A^{pq}$ ","date":"2022-03-14","objectID":"/introduction2la_l3/:1:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"Block Matrices and Block Multiplication $\\begin{bmatrix} I \u0026 0 \\cr -CA^{-1} \u0026 I \\end{bmatrix} \\begin{bmatrix} A \u0026 B \\cr C \u0026 D \\end{bmatrix} = \\begin{bmatrix} A \u0026 B \\cr 0 \u0026 D-CA^{-1}B \\end{bmatrix}$ Block elimination produces the Schur complement $D-CA^{-1}B$. ","date":"2022-03-14","objectID":"/introduction2la_l3/:1:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"2.5 Inverse Matrices Two sided inverse $A^{-1}A = AA^{-1}=I$ IMPORTANT: If A is invertible, then Ax = 0 can only have the zero solution $x = A^{-1}0 = 0$. OR: If Ax = 0 for a nonzero vector x, then A has no inverse. ","date":"2022-03-14","objectID":"/introduction2la_l3/:2:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"The Inverses of a Product AB If A and B are invertible then so is AB. Then inverse of a product AB is $(AB)^{-1}=B^{-1}A^{-1}$ Inverse of AB $(AB)B^{-1}A^{-1} = AIA^{-1} = AA^{-1}=I$ $(ABC)^{-1}= C^{-1}B^{-1}A^{-1}$ Inverse of an elimination matrix $E=\\begin{bmatrix}1\u00260\u00260 \\cr -5 \u0026 1 \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{bmatrix} \\quad and \\quad E^{-1} =\\begin{bmatrix}1\u00260\u00260 \\cr 5 \u0026 1 \u0026 0 \\cr 0 \u0026 0 \u0026 1 \\end{bmatrix}$ For square matrices, an inverse on one side is automatically an inverse on the other side. $F=\\begin{bmatrix}1\u00260\u00260 \\cr -0 \u0026 1 \u0026 0 \\cr 0 \u0026 -4 \u0026 1 \\end{bmatrix} \\quad and \\quad F^{-1} =\\begin{bmatrix}1\u00260\u00260 \\cr 0 \u0026 1 \u0026 0 \\cr 0 \u0026 4 \u0026 1 \\end{bmatrix}$ In elimination order F follows E. In reverse order $E^{-1}$ follows $F^{-1}$. $FE=\\begin{bmatrix}1\u00260\u00260 \\cr -5 \u0026 1 \u0026 0 \\cr 20 \u0026 -4 \u0026 1 \\end{bmatrix} \\quad \\text{is inverted by } E^{-1}F^{-1} =\\begin{bmatrix}1\u00260\u00260 \\cr 5 \u0026 1 \u0026 0 \\cr 0 \u0026 4 \u0026 1 \\end{bmatrix}$ $E^{-1}F^{-1}$ is quick. The multipliers 5, 4 fall into place below the diagonal of 1 ‚Äôs. ","date":"2022-03-14","objectID":"/introduction2la_l3/:2:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"Calculating $A^{-1}$ by Gauss-Jordan Elimination Each of the columns $x_1,x_2,x_3$ of $A^{-1}$ is multiplied by $A$ to produce a column of $I$: $AA^{-1} = A\\begin{bmatrix} x_1\u0026x_2\u0026x_3\\end{bmatrix} = \\begin{bmatrix} e_1\u0026e_2\u0026e_3\\end{bmatrix} = I$ $Ax_1=e_1=(1,0,0);Ax_2=e_2;\\cdots$ The Gauss-Jordan method computes $A^{-1}$ by solving all n equations together. $\\begin{bmatrix} K \u0026 e_1 \u0026 e_2 \u0026 e_3 \\end{bmatrix} =\\begin{bmatrix} K \u0026 I \\end{bmatrix} \\Rightarrow \\begin{bmatrix} I \u0026 x_1 \u0026 x_2 \u0026 x_3 \\end{bmatrix} =\\begin{bmatrix} I \u0026 K^{-1} \\end{bmatrix}$ ","date":"2022-03-14","objectID":"/introduction2la_l3/:2:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"Singular verus Invertible $A^{-1} \\text{ exists exactly when A has a full set of n pivots.(Row exchange allowed)}$ A triangular matrix is invertible if and only if no diagonal entries are zero. ","date":"2022-03-14","objectID":"/introduction2la_l3/:2:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"Recognizing an Invertible Matrix Diagonally dominant matrices are invertible.ÔºàUnnecessary and sufficient conditionÔºâ Diagonally dominant matrices are invertible. $|a_{ii}| \u003e \\sum_{j\\neq i}|a_{ij}|$ $\\text{Example: } \\begin{bmatrix} 3 \u0026 1 \u0026 1 \\cr 1 \u0026 3 \u0026 1 \\cr 1 \u0026 1 \u0026 3 \\end{bmatrix}$ ","date":"2022-03-14","objectID":"/introduction2la_l3/:2:4","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L3 Note","uri":"/introduction2la_l3/"},{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-12","objectID":"/introduction2la_l2/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"1. Mind Map 2. Reading Notes(2.2-2.3) ","date":"2022-03-12","objectID":"/introduction2la_l2/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"2.2 The Idea of Elimination A systematic way to solve linear equations: elimination GOAL: produce an upper triangular system The system solved from the bottom upwards: back substitution Pivot : first nonzero in the row that does the elimination Multiplier : (entry to eliminate) divided by (pivot) ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Breakdown of Elimination Failure: The method might ask us to divide by zero. Permanent failure with no solution. Failure with infinitely many solutions. Elimination leads to an equation $0 \\neq 0$ (no solution) or 0 = 0 (many solutions) Success comes with n pivots. But we may have to exchange the n equations. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Three Equations in Three Unknowns Goal: Forward elimination is complete from A to U. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Elimination from A to U Column 1. Use the first equation to create zeros below the first pivot. Column2. Use the new equation2 to create zeros below the second pivot. Columns 3 to n. Keep going to find all n pivots and the upper triangular U. ","date":"2022-03-12","objectID":"/introduction2la_l2/:1:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"2.3 Elimination Using Matrices ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Matrices times Vectors and Ax = b $Ax=b$ Ax is a combination of columns of A Components of Ax are dot products with rows of A. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Matrix Form of One Elimination Step $Ax=b: \\begin{bmatrix}2\u00264\u0026-2 \\cr 4\u00269\u0026-3 \\cr -2\u0026-3\u00267 \\end{bmatrix} \\begin{bmatrix} -1 \\cr 2 \\cr 2 \\end{bmatrix} =\\begin{bmatrix}2 \\cr 8 \\cr 10 \\end{bmatrix}$ First step: Subtract 2*Row1 from Row2 Elimination matrix is $E = \\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix}$ $b_{new} = Eb$ $\\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix} \\begin{bmatrix} 2 \\cr 8 \\cr 10 \\end{bmatrix} =\\begin{bmatrix}2 \\cr 4 \\cr 10 \\end{bmatrix}$ $\\begin{bmatrix}1\u00260\u00260 \\cr -2\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix} \\begin{bmatrix}b_1 \\cr b_2 \\cr b_3 \\end{bmatrix} = \\begin{bmatrix}b_1 \\cr -2b_1+b_2 \\cr b_3 \\end{bmatrix}$ I: identity matrix $\\begin{bmatrix}1\u00260\u00260 \\cr 0\u00261\u00260 \\cr 0\u00260\u00261 \\end{bmatrix}$ E :elementary matrix or elimination matrix $E_{ij}$ has extra nonzero entry $-l$ in the i,j position. Then $E_{ij}$ subtracts a multiple $l$ of row j from row i. $E_{31} = \\begin{bmatrix}1\u00260\u00260 \\cr 0\u00261\u00260 \\cr -l\u00260\u00261 \\end{bmatrix}$ The purpose of $E_{31}$ **is to produce a zero in the ( 3, 1) position of the matrix.** Products and inverses are especially clear for E‚Äôs. It is those two ideas that the book will use. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Matrix Multiplication Q: How do we multiply two matrices? $E(Ax) =Eb \\quad{also}\\quad (EA)x = Eb$ Associative law is true Commutative law is false E on the rights acts on the columns of A E on the left acts on the rows of A Matrix multiplication: $AB = A [b_1,b_2,b_3]=[Ab_1,Ab_2,Ab_3]$ ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Matrix $P_{ij}$ for a Row Exchange Permutation Matrix A row exchange is needed when zero is in the pivot position. $$ \\begin{bmatrix}1\u00260\u00260 \\cr 0\u00260\u00261 \\cr 0\u00261\u00260 \\end{bmatrix} \\begin{bmatrix}2\u00264\u00261 \\cr 0\u00260\u00263 \\cr 0\u00266\u00265 \\end{bmatrix} = \\begin{bmatrix}2\u00264\u00261 \\cr 0\u00266\u00265 \\cr 0\u00260\u00263 \\end{bmatrix} $$ Row Exchange Matrix $P_{ij}$ is the identity matrix with rows i and j reversed. ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:4","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"The Augmented Matrix $$ \\text{Augmented matrix} [A\\quad b] = \\begin{bmatrix} 2\u00264\u0026-1\u00262 \\cr 4\u00269\u0026-3\u00264 \\cr -2 \u0026-3\u00267\u002610 \\end{bmatrix} $$ ","date":"2022-03-12","objectID":"/introduction2la_l2/:2:5","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L2 Note","uri":"/introduction2la_l2/"},{"categories":["Studying"],"content":"Notes on a great open course: Linear Algebra.","date":"2022-03-10","objectID":"/introduction2la_l1/","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"Lecture 1: The geometry of linear equations 1. Mind Map 2. Reading Notes (1.1-2.1) Chapter 1 Introduction to Vectors $$ \\text{Linear combination} \\quad c\\vec{v}+ d\\vec{w}= c \\begin{bmatrix} 1\\cr 1\\end{bmatrix} +d \\begin{bmatrix}2\\cr 3\\end{bmatrix} =\\begin{bmatrix}c+2d\\cr c+3d\\end{bmatrix} $$ 1.1 Vector addition $v+w$ and linear combinations $cv+dw$. 1.2 The dot product $v ¬∑ w$ of two vectors and the length $\\lVert v \\rVert = \\sqrt{v \\cdot v}$ 1.3 Matrices A, linear equations $Ax=b$, solutions $x = A^{-1} b.$ ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.1 Vectors and Linear Combinations Vector Addition Scalar Multiplication ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.2 Lengths and Dot Products Dot product / Inner Product is the number of $\\vec{v}\\cdot \\vec{w}$ $\\vec{v}= (v_1,v_2), \\vec{w}= (w_1,w_2)$ $\\vec{v}\\cdot \\vec{w} = v_1w_1 +v_2w_2$ Dot product = 0 ‚Üí Perpendicular vectors Order makes no difference. Lengths and Unit Vectors length = $\\lVert v \\rVert = \\sqrt{v \\cdot v} = (v_1^2 + v_2^2 \\cdots +v_n^2)^{1/2}$ unit vector u is a vector whose length equals one Unit vector $u = v / \\lVert v\\rVert$ Cosine Formula: If v and w are nonzero vectors then $\\frac{v \\cdot w}{\\lVert v\\rVert\\lVert w\\rVert} = cos\\theta$ Schwarz Inequality $|v \\cdot w| \\leq \\lVert v\\rVert\\lVert w\\rVert$ Triangle Inequality $\\lVert v+w\\rVert \\leq \\lVert v\\rVert + \\lVert w\\rVert$ Geometric mean $\\leq$ Arithmetic mean : $\\sqrt{xy} \\leq \\frac{x+y}{2}$ *Cosine Formula: $cosine = v\\prime * w / (norm(v)norm(w))$ The arc cosine: angle = acos(cosine) ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:2","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"1.3 Matrices Matrix times vector: Combination b of columns of A (matrix A acts on the vector x) $$ Ax = \\begin{bmatrix}1\u00260\u00260 \\cr -1\u00261\u00260\\cr 0\u0026-1\u00261 \\end{bmatrix} \\begin{bmatrix}x_1 \\cr x_2\\cr x_3 \\end{bmatrix} = \\begin{bmatrix}x_1 \\cr x_2-x_1 \\cr x_3-x_2 \\end{bmatrix} $$ Ax is also dot products with rows $$ Ax = \\begin{bmatrix}1\u00260\u00260 \\cr -1\u00261\u00260\\cr 0\u0026-1\u00261 \\end{bmatrix} \\begin{bmatrix}x_1 \\cr x_2\\cr x_3 \\end{bmatrix} = \\begin{bmatrix}x_1 \\cr x_2-x_1 \\cr x_3-x_2 \\end{bmatrix}= \\begin{bmatrix}(1,0,0)\\cdot(x_1,x_2,x_3) \\cr (-1,1,0)\\cdot(x_1,x_2,x_3) \\cr (0,-1,1)\\cdot(x_1,x_2,x_3) \\end{bmatrix} $$ Linear Equations new viewpoint: Q: Which combination of u, v, w produces a particular vector b? Inverse problem: how to find the input x that gives the desired output b = Ax $\\text{Equations: }Ax=b\\quad \\text{Solution: }x=A^{-1}b$ matrix A is ‚Äúinvertible‚Äù üí° The Inverse Matrix ???: centered difference Worked examples **1.3C** Independence and Dependence Independent columns: Ax=0 has one solution. A is an invertible matrix. Dependent columns: Cx = 0 has many solutions. C is a singular matrix. ","date":"2022-03-10","objectID":"/introduction2la_l1/:0:3","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"Chapter 2 Solving Linear Equations ","date":"2022-03-10","objectID":"/introduction2la_l1/:1:0","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Studying"],"content":"2.1 Vectors and Linear Equations Two equations, two unknowns Row Picture Column Picture Coefficient matrix The Matrix Form of the Equations Multiplication by rows. (dot products) $Ax = \\begin{bmatrix}(row1)\\cdot x \\cr (row2)\\cdot x \\cr (row3)\\cdot x \\end{bmatrix}$ Multiplication by columns (combination of columns) $Ax = x(column1) +y(column2) +z(column3)$ ","date":"2022-03-10","objectID":"/introduction2la_l1/:1:1","tags":["Math"],"title":"MIT 18.06(Linear Algebra) L1 Note","uri":"/introduction2la_l1/"},{"categories":["Watching"],"content":"Notes on Youtube channel: AI coffe break Letitia.","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"How to check if a neural network has learned a specific phenomenon? ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:0:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"Question: How do we check if a neural network trained on task A has learned a phenomenon specific to task B? To NLP, the task is called probing/diagnostic classifier/probing task. ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:1:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"How do we do probing? Freeze the parameters of Input layer and Hidden layers, Replace the last layers to Probing layers, Train the Probing layers on a small dataset If task A is Masked word prediction and task B is Sentiment analysis, then: ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:2:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"Most recently research paper: Information-Theoretic Probing with Minimum Description Length on ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:3:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Watching"],"content":"More info Link: https://www.youtube.com/watch?v=fL22NAtMNYo ","date":"2022-02-16","objectID":"/ai_coffe_break_notes1/:4:0","tags":["ML","Notes"],"title":"AI coffe break: notes1","uri":"/ai_coffe_break_notes1/"},{"categories":["Studying"],"content":"A three moths schedule for Machine Learning Foundations.","date":"2022-01-25","objectID":"/3_month_ml/","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"Build Your Machine Learning on the Rock A three moths schedule for Machine Learning Foundations. ","date":"2022-01-25","objectID":"/3_month_ml/:0:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"1. Introduction Nowadays Machine learning and Deep learning are widely used in all subjects, both industry and research organizations take huge effort to apply these methods. I major in Information Management and Information System for my undergraudate study, and had some research experience related to Natural language Processing. I‚Äôm quite excited to study in the area of AI, especially in NLP/CLÔºåsince there are so many great contributors and huge progressed they‚Äôve achieved. More is Less and Less is More! I believe that no matter how fast this field is growing, the most important thing we can do is to build a good foundation if we want to dive into it. ","date":"2022-01-25","objectID":"/3_month_ml/:1:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2. Planning Thanks to an UP(NLP‰ªéÂÖ•Èó®Âà∞ÊîæÂºÉ) share his great learning experience bilibili, I decided to follow his guide and start my 3 moths ML schedule. There 6 modules in this guide for about 3 months. ","date":"2022-01-25","objectID":"/3_month_ml/:2:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.1 Prerequisites Week 1: Python Week 2: Python for data analysis Week3: Math(Linear Algebra, Calculus, Probability) ","date":"2022-01-25","objectID":"/3_month_ml/:2:1","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.2 ML Theory Book : Statistical learning methods by Hang Li Must-learn chapter Introduction Perceptron Naive Bayes Decision Tree Logistic Regression and MaxEnt Boosted Trees XGBoost Reference Blog: Jianping Liu‚Äôs Blog Principle Can derive formulas by yourself. Don‚Äòt read every chapter in this book except for the must-learns'. Don‚Äòt reinvent wheels ","date":"2022-01-25","objectID":"/3_month_ml/:2:2","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.3 ML Practice Alibaba Cloud TIANCHI Industrial steam volume forecast Repeat purchase prediction for Tmall users O2O Coupon Forecast Ali cloud security malicious program detection ","date":"2022-01-25","objectID":"/3_month_ml/:2:3","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.4 DL Theory CV - cs231n P1-P22 HW1,HW2 NLP - cs224n P1-P5 \u0026 P8,P9,P11 HW Pytorch Learning Bilibili Âàò‰∫åÂ§ß‰∫∫ A repo ","date":"2022-01-25","objectID":"/3_month_ml/:2:4","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.5 DL Practice NLP Ali Cloud Tianchi: News Text Categories CV Ali Cloud Tianchi: Streetscape symbol recognition ","date":"2022-01-25","objectID":"/3_month_ml/:2:5","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"2.6 Interview Questions Book: The Quest for Machine Learning ","date":"2022-01-25","objectID":"/3_month_ml/:2:6","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"},{"categories":["Studying"],"content":"3. Note Every time I have finished some in this schedule, I‚Äôll post it to my blog. ","date":"2022-01-25","objectID":"/3_month_ml/:3:0","tags":["ML","Foundation"],"title":"Build Your Machine Learning on the Rock","uri":"/3_month_ml/"}]